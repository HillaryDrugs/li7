{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnUjSq/qj87xiEC/Gy3zXx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HillaryDrugs/li7/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zOZF91Qw8kz",
        "outputId": "bc6893a8-67a3-49f6-b1f4-d4cbccd5b797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Train size: 4457 | Test size: 1115\n",
            "label\n",
            "ham     4825\n",
            "spam     747\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "Vocab size: 7920\n",
            "Train batches: 140\n",
            "Test  batches: 35\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:07<00:00, 18.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Train Loss: 0.2558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:08<00:00, 16.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5 - Train Loss: 0.0724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:08<00:00, 16.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5 - Train Loss: 0.0385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:07<00:00, 18.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5 - Train Loss: 0.0194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:08<00:00, 16.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5 - Train Loss: 0.0167\n",
            "\n",
            "================ RESULTS ================\n",
            "Test Accuracy: 0.9812\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Ham       0.99      0.99      0.99       966\n",
            "        Spam       0.94      0.91      0.93       149\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.97      0.95      0.96      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            "\n",
            "\n",
            "================ EXAMPLES ================\n",
            "Message: WIN a brand new car now!\n",
            "Prediction: HAM âœ…\n",
            "\n",
            "Message: Can we meet tomorrow for coffee?\n",
            "Prediction: HAM âœ…\n",
            "\n",
            "Message: URGENT! Your account was hacked.\n",
            "Prediction: HAM âœ…\n",
            "\n",
            "Message: Ok I'm home, text me when you arrive.\n",
            "Prediction: HAM âœ…\n",
            "\n",
            "Message: You have won $5000 cash. Call now to receive your reward.\n",
            "Prediction: SPAM ðŸš¨\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ======================\n",
        "# 1. INSTALL DEPENDENCIES\n",
        "# ======================\n",
        "!pip install scikit-learn tensorflow -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# ======================\n",
        "# 2. DEVICE (GPU OR CPU)\n",
        "# ======================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ======================\n",
        "# 3. LOAD & PREPARE DATA\n",
        "# ======================\n",
        "# Make sure /content/spam.csv exists in Colab\n",
        "df = pd.read_csv(\"/content/spam.csv\", encoding=\"cp1252\")\n",
        "\n",
        "# keep only label/text cols\n",
        "df = df[[\"v1\", \"v2\"]].rename(columns={\"v1\": \"label\", \"v2\": \"text\"})\n",
        "\n",
        "# map ham/spam -> 0/1\n",
        "label_map = {\"ham\": 0, \"spam\": 1}\n",
        "df[\"label_id\"] = df[\"label\"].map(label_map)\n",
        "\n",
        "texts = df[\"text\"].tolist()\n",
        "labels = df[\"label_id\"].tolist()\n",
        "\n",
        "# split dataset\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    texts,\n",
        "    labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"Train size: {len(train_texts)} | Test size: {len(test_texts)}\")\n",
        "print(df[\"label\"].value_counts(), \"\\n\")\n",
        "\n",
        "# ======================\n",
        "# 4. TOKENIZE & PAD TEXT\n",
        "# ======================\n",
        "max_words = 10000   # vocab cap\n",
        "max_len = 50        # sequence length (you can make it 60/80 if you want)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences  = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "X_train = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "X_test  = pad_sequences(test_sequences,  maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "y_train = torch.tensor(train_labels, dtype=torch.long)\n",
        "y_test  = torch.tensor(test_labels,  dtype=torch.long)\n",
        "\n",
        "vocab_size = min(max_words, len(tokenizer.word_index) + 1)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "# ======================\n",
        "# 5. PYTORCH DATASET / DATALOADER\n",
        "# ======================\n",
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.y = y\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = SpamDataset(X_train, y_train)\n",
        "test_dataset  = SpamDataset(X_test,  y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Test  batches: {len(test_loader)}\\n\")\n",
        "\n",
        "# ======================\n",
        "# 6. CNN MODEL (TRAINABLE EMBEDDINGS)\n",
        "# ======================\n",
        "class SpamCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=100, num_filters=100, filter_sizes=[3,4,5], dropout=0.5):\n",
        "        super(SpamCNN, self).__init__()\n",
        "\n",
        "        # trainable embedding (random init)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # multiple convolution branches with different n-gram sizes\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(\n",
        "                in_channels=embed_dim,\n",
        "                out_channels=num_filters,\n",
        "                kernel_size=k\n",
        "            )\n",
        "            for k in filter_sizes\n",
        "        ])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(num_filters * len(filter_sizes), 2)  # ham/spam\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch, max_len] (token ids)\n",
        "        x = self.embedding(x)          # [batch, max_len, embed_dim]\n",
        "        x = x.permute(0, 2, 1)         # [batch, embed_dim, max_len] for Conv1d\n",
        "\n",
        "        conv_outs = []\n",
        "        for conv in self.convs:\n",
        "            c = conv(x)                # [batch, num_filters, L']\n",
        "            c = torch.relu(c)\n",
        "            c = torch.max(c, dim=2).values  # global max pool -> [batch, num_filters]\n",
        "            conv_outs.append(c)\n",
        "\n",
        "        x = torch.cat(conv_outs, dim=1)      # [batch, num_filters * len(filter_sizes)]\n",
        "        x = self.dropout(x)\n",
        "        logits = self.fc(x)                  # [batch, 2]\n",
        "        return logits\n",
        "\n",
        "model = SpamCNN(vocab_size=vocab_size).to(device)\n",
        "\n",
        "# ======================\n",
        "# 7. TRAINING SETUP\n",
        "# ======================\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "epochs = 5  # lower to 3 or 2 if needed for speed\n",
        "\n",
        "# ======================\n",
        "# 8. TRAIN LOOP\n",
        "# ======================\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)          # [batch, 2]\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# ======================\n",
        "# 9. EVALUATION\n",
        "# ======================\n",
        "model.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        y_true.extend(y_batch.numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "print(\"\\n================ RESULTS ================\")\n",
        "print(f\"Test Accuracy: {acc:.4f}\\n\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Ham\", \"Spam\"]))\n",
        "\n",
        "# ======================\n",
        "# 10. PREDICTION FUNCTION\n",
        "# ======================\n",
        "def predict_message(model, tokenizer, text, max_len=50):\n",
        "    model.eval()\n",
        "\n",
        "    seq = tokenizer.texts_to_sequences([text])\n",
        "    padded = pad_sequences(seq, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "    tensor = torch.tensor(padded, dtype=torch.long).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(tensor)                  # [1, 2]\n",
        "        pred = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    return \"SPAM ðŸš¨\" if pred == 1 else \"HAM âœ…\"\n",
        "\n",
        "# quick manual test\n",
        "samples = [\n",
        "    \"WIN a brand new car now!\",\n",
        "    \"Can we meet tomorrow for coffee?\",\n",
        "    \"URGENT! Your account was hacked.\",\n",
        "    \"Ok I'm home, text me when you arrive.\",\n",
        "    \"You have won $5000 cash. Call now to receive your reward.\"\n",
        "]\n",
        "\n",
        "print(\"\\n================ EXAMPLES ================\")\n",
        "for msg in samples:\n",
        "    print(f\"Message: {msg}\")\n",
        "    print(\"Prediction:\", predict_message(model, tokenizer, msg))\n",
        "    print()\n"
      ]
    }
  ]
}