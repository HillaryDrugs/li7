{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM60GU417ddbrSRshHH3bp3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HillaryDrugs/li7/blob/main/BiLTSM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCq6uU9SCw3k",
        "outputId": "1000f41f-7043-461b-f187-91644bb0ef6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Train size: 4457 | Test size: 1115\n",
            "label\n",
            "ham     4825\n",
            "spam     747\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "Vocab size: 7920\n",
            "Train batches: 140\n",
            "Test  batches: 35\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:08<00:00, 17.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Train Loss: 0.2703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:06<00:00, 21.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5 - Train Loss: 0.1070\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:07<00:00, 18.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5 - Train Loss: 0.0502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:06<00:00, 20.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5 - Train Loss: 0.0286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:07<00:00, 18.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5 - Train Loss: 0.0159\n",
            "\n",
            "================ RESULTS ================\n",
            "Test Accuracy: 0.9776\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Ham       0.98      0.99      0.99       966\n",
            "        Spam       0.94      0.89      0.91       149\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.96      0.94      0.95      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            "\n",
            "\n",
            "================ EXAMPLES ================\n",
            "Message: WIN a brand new car now!\n",
            "Prediction: HAM âœ…\n",
            "\n",
            "Message: Can we meet tomorrow for coffee?\n",
            "Prediction: HAM âœ…\n",
            "\n",
            "Message: URGENT! Your account was hacked.\n",
            "Prediction: HAM âœ…\n",
            "\n",
            "Message: Ok I'm home, text me when you arrive.\n",
            "Prediction: HAM âœ…\n",
            "\n",
            "Message: You have won $5000 cash. Call now to receive your reward.\n",
            "Prediction: SPAM ðŸš¨\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ======================\n",
        "# 1. INSTALL DEPENDENCIES\n",
        "# ======================\n",
        "!pip install scikit-learn tensorflow -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# ======================\n",
        "# 2. DEVICE (GPU OR CPU)\n",
        "# ======================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ======================\n",
        "# 3. LOAD & PREPARE DATA\n",
        "# ======================\n",
        "# Make sure /content/spam.csv exists in Colab\n",
        "df = pd.read_csv(\"/content/spam.csv\", encoding=\"cp1252\")\n",
        "\n",
        "# Keep only the label and text columns\n",
        "df = df[[\"v1\", \"v2\"]].rename(columns={\"v1\": \"label\", \"v2\": \"text\"})\n",
        "\n",
        "# Map ham/spam -> 0/1\n",
        "label_map = {\"ham\": 0, \"spam\": 1}\n",
        "df[\"label_id\"] = df[\"label\"].map(label_map)\n",
        "\n",
        "texts = df[\"text\"].tolist()\n",
        "labels = df[\"label_id\"].tolist()\n",
        "\n",
        "# Train/test split (same style as before for fairness)\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    texts,\n",
        "    labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"Train size: {len(train_texts)} | Test size: {len(test_texts)}\")\n",
        "print(df[\"label\"].value_counts(), \"\\n\")\n",
        "\n",
        "# ======================\n",
        "# 4. TOKENIZE & PAD TEXT\n",
        "# ======================\n",
        "max_words = 10000   # vocab size cap\n",
        "max_len = 50        # message length cap\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences  = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "X_train = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "X_test  = pad_sequences(test_sequences,  maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "y_train = torch.tensor(train_labels, dtype=torch.long)\n",
        "y_test  = torch.tensor(test_labels,  dtype=torch.long)\n",
        "\n",
        "vocab_size = min(max_words, len(tokenizer.word_index) + 1)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "# ======================\n",
        "# 5. DATASET / DATALOADER\n",
        "# ======================\n",
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.y = y\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = SpamDataset(X_train, y_train)\n",
        "test_dataset  = SpamDataset(X_test,  y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Test  batches: {len(test_loader)}\\n\")\n",
        "\n",
        "# ======================\n",
        "# 6. BiLSTM MODEL\n",
        "# ======================\n",
        "class SpamBiLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=100, hidden_dim=64, num_layers=1, dropout=0.5):\n",
        "        super(SpamBiLSTM, self).__init__()\n",
        "\n",
        "        # Trainable embeddings (random init)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # Bidirectional LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=0.0 if num_layers == 1 else dropout\n",
        "        )\n",
        "\n",
        "        # Because it's bidirectional, hidden_dim * 2 comes out\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, 2)  # 2 classes: ham/spam\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch, max_len]\n",
        "        x = self.embedding(x)  # [batch, max_len, embed_dim]\n",
        "\n",
        "        # lstm_out: [batch, max_len, hidden_dim*2] because bidirectional=True\n",
        "        # (h_n, c_n) we don't actually need all time steps for classification,\n",
        "        # we can take the last hidden state from both directions.\n",
        "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "        # h_n shape: [num_layers * 2, batch, hidden_dim]\n",
        "        # take last layer's forward and backward hidden states and concat\n",
        "        # forward is h_n[-2], backward is h_n[-1] when bidirectional\n",
        "        if self.lstm.bidirectional:\n",
        "            h_forward = h_n[-2]\n",
        "            h_backward = h_n[-1]\n",
        "            h_combined = torch.cat((h_forward, h_backward), dim=1)  # [batch, hidden_dim*2]\n",
        "        else:\n",
        "            h_combined = h_n[-1]  # [batch, hidden_dim]\n",
        "\n",
        "        x = self.dropout(h_combined)\n",
        "        logits = self.fc(x)  # [batch, 2]\n",
        "        return logits\n",
        "\n",
        "model = SpamBiLSTM(vocab_size=vocab_size).to(device)\n",
        "\n",
        "# ======================\n",
        "# 7. TRAINING SETUP\n",
        "# ======================\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "epochs = 5  # you can set 3 if you want faster\n",
        "\n",
        "# ======================\n",
        "# 8. TRAIN LOOP\n",
        "# ======================\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)        # [batch, 2]\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# ======================\n",
        "# 9. EVALUATION\n",
        "# ======================\n",
        "model.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "\n",
        "        logits = model(X_batch)              # [batch, 2]\n",
        "        preds = torch.argmax(logits, dim=1)  # [batch]\n",
        "\n",
        "        y_true.extend(y_batch.numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "print(\"\\n================ RESULTS ================\")\n",
        "print(f\"Test Accuracy: {acc:.4f}\\n\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Ham\", \"Spam\"]))\n",
        "\n",
        "# ======================\n",
        "# 10. PREDICTION FUNCTION\n",
        "# ======================\n",
        "def predict_message(model, tokenizer, text, max_len=50):\n",
        "    model.eval()\n",
        "\n",
        "    seq = tokenizer.texts_to_sequences([text])\n",
        "    padded = pad_sequences(seq, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "    tensor = torch.tensor(padded, dtype=torch.long).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(tensor)                  # [1, 2]\n",
        "        pred = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    return \"SPAM ðŸš¨\" if pred == 1 else \"HAM âœ…\"\n",
        "\n",
        "# Try a few manual samples\n",
        "samples = [\n",
        "    \"WIN a brand new car now!\",\n",
        "    \"Can we meet tomorrow for coffee?\",\n",
        "    \"URGENT! Your account was hacked.\",\n",
        "    \"Ok I'm home, text me when you arrive.\",\n",
        "    \"You have won $5000 cash. Call now to receive your reward.\"\n",
        "]\n",
        "\n",
        "print(\"\\n================ EXAMPLES ================\")\n",
        "for msg in samples:\n",
        "    print(f\"Message: {msg}\")\n",
        "    print(\"Prediction:\", predict_message(model, tokenizer, msg))\n",
        "    print()\n"
      ]
    }
  ]
}