{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPA5wp9hdbxaVyaC7uvApga",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HillaryDrugs/li7/blob/main/Glove_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekBwsEBHs82C",
        "outputId": "b22b27e4-f7fe-43d3-f880-c66a08186ca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Train size: 4457 | Test size: 1115\n",
            "label\n",
            "ham     4825\n",
            "spam     747\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "Loading GloVe embeddings (100d)... this may take a moment.\n",
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "Train batches: 140\n",
            "Test batches : 35\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:02<00:00, 54.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Train Loss: 0.4383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:02<00:00, 60.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5 - Train Loss: 0.1967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:02<00:00, 60.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5 - Train Loss: 0.1040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:03<00:00, 43.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5 - Train Loss: 0.0938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:02<00:00, 59.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5 - Train Loss: 0.0790\n",
            "\n",
            "================ RESULTS ================\n",
            "Test Accuracy: 0.9704\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Ham       0.99      0.98      0.98       966\n",
            "        Spam       0.87      0.91      0.89       149\n",
            "\n",
            "    accuracy                           0.97      1115\n",
            "   macro avg       0.93      0.95      0.94      1115\n",
            "weighted avg       0.97      0.97      0.97      1115\n",
            "\n",
            "\n",
            "================ EXAMPLES ================\n",
            "Message: WIN a brand new car now!\n",
            "Prediction: HAM âœ…\n",
            "\n",
            "Message: Can we meet tomorrow for coffee?\n",
            "Prediction: HAM âœ…\n",
            "\n",
            "Message: URGENT! Your account was hacked.\n",
            "Prediction: HAM âœ…\n",
            "\n",
            "Message: Ok I'm home, text me when you arrive.\n",
            "Prediction: HAM âœ…\n",
            "\n",
            "Message: You have won $5000 cash. Call now to receive your reward.\n",
            "Prediction: SPAM ðŸš¨\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ======================\n",
        "# 1. INSTALL DEPENDENCIES\n",
        "# ======================\n",
        "!pip install --upgrade gensim -q\n",
        "!pip install scikit-learn tensorflow -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "import gensim.downloader as api\n",
        "from tqdm import tqdm\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# ======================\n",
        "# 2. DEVICE (GPU OR CPU)\n",
        "# ======================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ======================\n",
        "# 3. LOAD & PREPARE DATA\n",
        "# ======================\n",
        "# Make sure spam.csv is uploaded to /content/spam.csv in Colab\n",
        "df = pd.read_csv(\"/content/spam.csv\", encoding=\"cp1252\")\n",
        "\n",
        "# keep only label/text columns and rename\n",
        "df = df[[\"v1\", \"v2\"]].rename(columns={\"v1\": \"label\", \"v2\": \"text\"})\n",
        "\n",
        "# map ham/spam -> 0/1\n",
        "label_map = {\"ham\": 0, \"spam\": 1}\n",
        "df[\"label_id\"] = df[\"label\"].map(label_map)\n",
        "\n",
        "texts = df[\"text\"].tolist()          # list of SMS strings\n",
        "labels = df[\"label_id\"].tolist()     # list of 0/1\n",
        "\n",
        "# split train/test\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    texts,\n",
        "    labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"Train size: {len(train_texts)} | Test size: {len(test_texts)}\")\n",
        "print(df[\"label\"].value_counts(), \"\\n\")\n",
        "\n",
        "# ======================\n",
        "# 4. LOAD GloVe\n",
        "# ======================\n",
        "print(\"Loading GloVe embeddings (100d)... this may take a moment.\")\n",
        "glove = api.load(\"glove-wiki-gigaword-100\")  # each word -> 100-dim vector\n",
        "\n",
        "# ======================\n",
        "# 5. TOKENIZE & PAD TEXT\n",
        "# ======================\n",
        "max_words = 10000   # only keep top 10k words\n",
        "max_len = 50        # cut / pad each message to 50 tokens\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences  = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "X_train = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "X_test  = pad_sequences(test_sequences,  maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "y_train = torch.tensor(train_labels, dtype=torch.long)\n",
        "y_test  = torch.tensor(test_labels,  dtype=torch.long)\n",
        "\n",
        "# ======================\n",
        "# 6. BUILD EMBEDDING MATRIX\n",
        "# ======================\n",
        "embedding_dim = 100  # because glove-wiki-gigaword-100 is 100d\n",
        "word_index = tokenizer.word_index  # word -> index\n",
        "num_words = min(max_words, len(word_index) + 1)\n",
        "\n",
        "# matrix shape: (vocab_size, embedding_dim)\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim), dtype=np.float32)\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_words:\n",
        "        continue\n",
        "    if word in glove:  # if this word exists in GloVe vocab\n",
        "        embedding_matrix[i] = glove[word]\n",
        "    # else it stays as zeros (unknown / OOV)\n",
        "\n",
        "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "\n",
        "# ======================\n",
        "# 7. PYTORCH DATASET + DATALOADER\n",
        "# ======================\n",
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        # X is numpy -> convert to torch LongTensor\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.y = y  # already a torch tensor\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = SpamDataset(X_train, y_train)\n",
        "test_dataset  = SpamDataset(X_test,  y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Test batches : {len(test_loader)}\\n\")\n",
        "\n",
        "# ======================\n",
        "# 8. MODEL (EMBED + LSTM + FC)\n",
        "# ======================\n",
        "class SpamClassifier(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim=64):\n",
        "        super(SpamClassifier, self).__init__()\n",
        "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
        "\n",
        "        # Embedding layer initialized with GloVe\n",
        "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.embedding.weight = nn.Parameter(embedding_matrix)\n",
        "        self.embedding.weight.requires_grad = False  # freeze GloVe weights\n",
        "\n",
        "        # LSTM reads the sequence of embeddings\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Final linear layer -> 2 classes (ham, spam)\n",
        "        self.fc = nn.Linear(hidden_dim, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch, max_len] of token IDs\n",
        "        x = self.embedding(x)         # -> [batch, max_len, embed_dim]\n",
        "        _, (hidden, _) = self.lstm(x) # hidden: [1, batch, hidden_dim]\n",
        "        hidden_last = hidden[-1]      # [batch, hidden_dim]\n",
        "        out = self.fc(hidden_last)    # [batch, 2]\n",
        "        return out\n",
        "\n",
        "model = SpamClassifier(embedding_matrix).to(device)\n",
        "\n",
        "# ======================\n",
        "# 9. TRAINING SETUP\n",
        "# ======================\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "epochs = 5  # you can lower to 3 or 2 if it's slow\n",
        "\n",
        "# ======================\n",
        "# 10. TRAIN LOOP\n",
        "# ======================\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)          # [batch, 2]\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# ======================\n",
        "# 11. EVALUATION\n",
        "# ======================\n",
        "model.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "\n",
        "        outputs = model(X_batch)              # [batch, 2]\n",
        "        preds = torch.argmax(outputs, dim=1)  # [batch]\n",
        "\n",
        "        y_true.extend(y_batch.numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "# metrics\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "print(\"\\n================ RESULTS ================\")\n",
        "print(f\"Test Accuracy: {acc:.4f}\\n\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Ham\", \"Spam\"]))\n",
        "\n",
        "# ======================\n",
        "# 12. PREDICTION FUNCTION\n",
        "# ======================\n",
        "def predict_message(model, tokenizer, text, max_len=50):\n",
        "    model.eval()\n",
        "\n",
        "    # convert text -> tokens -> padded sequence\n",
        "    seq = tokenizer.texts_to_sequences([text])\n",
        "    padded = pad_sequences(seq, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "    tensor = torch.tensor(padded, dtype=torch.long).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(tensor)                 # [1, 2]\n",
        "        pred = torch.argmax(output, dim=1).item()\n",
        "\n",
        "    return \"SPAM ðŸš¨\" if pred == 1 else \"HAM âœ…\"\n",
        "\n",
        "# quick sanity check on some messages\n",
        "samples = [\n",
        "    \"WIN a brand new car now!\",\n",
        "    \"Can we meet tomorrow for coffee?\",\n",
        "    \"URGENT! Your account was hacked.\",\n",
        "    \"Ok I'm home, text me when you arrive.\",\n",
        "    \"You have won $5000 cash. Call now to receive your reward.\"\n",
        "]\n",
        "\n",
        "print(\"\\n================ EXAMPLES ================\")\n",
        "for msg in samples:\n",
        "    print(f\"Message: {msg}\")\n",
        "    print(\"Prediction:\", predict_message(model, tokenizer, msg))\n",
        "    print()\n"
      ]
    }
  ]
}